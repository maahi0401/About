{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maahi0401/About/blob/main/Interview_Prep_4/interview_prep_simulation_scenarios_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNFVxsQIoRWF"
      },
      "source": [
        "# Module 4: Feature Engineering & Dimensionality Reduction\n",
        "\n",
        "## üéØ Interview Simulation Scenarios\n",
        "**Context:** This section transitions from theory to application, simulating the pressure and problem-solving flow of a technical data science interview.\n",
        "\n",
        "### üõ†Ô∏è Strategic Approach for Scenarios\n",
        "When tackling these problems, keep the **\"STAR\"** method in mind for your explanations:\n",
        "* **Situation:** Briefly describe the data problem.\n",
        "* **Task:** Identify which feature engineering/reduction technique is needed.\n",
        "* **Action:** Write the code and explain your parameter choices (e.g., why `StandardScaler`?).\n",
        "* **Result:** Interpret the output (e.g., \"This reduced our feature space by 60% while retaining 95% variance\").\n",
        "\n",
        "### üü¢ Warm-up: The Scaling Decision (5-10 min)\n",
        "**Scenario:** You are given a dataset with two features: `Age` (0‚Äì100) and `Annual Income` ($0‚Äì$1,000,000). You plan to use K-Means clustering.\n",
        "* **Problem:** What happens if you don't scale? Which scaler would you choose?\n",
        "* **Goal:** Demonstrate an understanding of distance-based algorithms.\n",
        "\n",
        "### üü° Intermediate: The Redundancy Filter (10-15 min)\n",
        "**Scenario:** You have a dataset with 50 features. A quick correlation matrix shows that 10 features have a correlation > 0.95 with each other.\n",
        "* **Problem:** Implement a function to automatically identify and drop one feature from each highly correlated pair.\n",
        "* **Goal:** Show efficiency in automated feature selection.\n",
        "\n",
        "### üü† Advanced: PCA for High-Dimensional Noise (15-20 min)\n",
        "**Scenario:** You are working with genomic data (thousands of features, few samples). The model is overfitting severely.\n",
        "* **Problem:** Use PCA to reduce the feature space. How do you determine if you've removed \"noise\" or \"signal\"?\n",
        "* **Goal:** Defend the use of a Scree Plot and cumulative variance thresholds.\n",
        "\n",
        "### üî¥ Challenge: The Production Pipeline (20-30 min)\n",
        "**Scenario:** You've built a t-SNE visualization that shows perfect class separation. Your manager asks you to put this t-SNE transformation into the real-time production pipeline for new incoming data.\n",
        "* **Problem:** Explain why this is or isn't possible. Propose an alternative architecture (e.g., using PCA or a small Autoencoder).\n",
        "* **Goal:** Demonstrate architectural knowledge and awareness of t-SNE's limitations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a266oJGwoRWJ"
      },
      "source": [
        "## SetupRun this cell first to import all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6h6REoLoRWJ",
        "outputId": "2ce03368-a39a-4dd8-9bad-558e282b5bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_iris, load_wine, fetch_california_housing\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"‚úì Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmx5b46ioRWL"
      },
      "outputs": [],
      "source": [
        "# Buggy code from junior data scientistfrom sklearn.datasets import load_irisfrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAiris = load_iris()X = iris.datay = iris.target# They did this:scaler = StandardScaler()X_scaled = scaler.fit_transform(X)# Then later, on test data:X_test = X[:10]  # Pretend this is test dataX_test_scaled = scaler.fit_transform(X_test)  # ‚ùå MISTAKE HEREpca = PCA(n_components=2)X_pca = pca.fit_transform(X_test_scaled)print(\"Transformed test data shape:\", X_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_J7n9tooRWK"
      },
      "source": [
        "# Problem 1.1: Detect the Scaling Mistake üü¢\n",
        "\n",
        "## üéØ Interview Prep: Debugging Scenario\n",
        "**Context:** Identifying common pitfalls in the data preprocessing pipeline that lead to data leakage or model degradation.\n",
        "\n",
        "### üìö The Buggy Code\n",
        "Imagine a junior developer presents the following snippet for scaling a training and test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc5ycDBWoRWL"
      },
      "source": [
        "**Your Task:**\n",
        "\n",
        "Identify the mistake in the code above2. Explain WHY it's wrong3. Fix it\n",
        "\n",
        "**Write your answer below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrfxVlRCoRWL"
      },
      "outputs": [],
      "source": [
        "# Your answer:\n",
        "\n",
        "# 1. What's the mistake?\n",
        "\n",
        "# ANSWER:\n",
        "\n",
        "# 2. Why is it wrong?\n",
        "\n",
        "# ANSWER:\n",
        "\n",
        "# 3. Fixed code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp6X3d0WoRWM"
      },
      "source": [
        "# Solution: Problem 1.1 - The Scaling Mistake üü¢\n",
        "\n",
        "## 1. Identify the Mistake\n",
        "The mistake in the code is the use of **`.fit_transform(X_test)`**.\n",
        "\n",
        "## 2. Explain WHY it is wrong\n",
        "In a technical interview, you want to frame this through the lens of **Data Leakage**:\n",
        "\n",
        "* **Learning Unseen Statistics:** When you call `fit_transform()` on the test set, the scaler calculates a *new* mean ($\\mu$) and standard deviation ($\\sigma$) based solely on the test data.\n",
        "* **The Golden Rule:** The test set must simulate \"future,\" unseen data. In a real-world production environment, you wouldn't have the entire \"future\" dataset to calculate a mean from.\n",
        "* **Inconsistent Transformation:** If the training set and test set have slightly different distributions, the same raw value (e.g., a \"Height\" of 180cm) would be mapped to different scaled values (e.g., 0.5 vs 0.7). This confuses the model because the numerical inputs no longer represent the same physical reality it learned during training.\n",
        "\n",
        "\n",
        "## 3. The Fix\n",
        "You should **`fit`** your scaler on the training data only. This \"locks in\" the parameters. You then apply those parameters to any other data (test set or new production samples) using **`transform`**.\n",
        "\n",
        "```python\n",
        "# The Correct Implementation\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 1. Fit to training data AND transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# 2. ONLY transform the test data (using parameters from X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ScHLTFsoRWM"
      },
      "source": [
        "# Problem 1.2: The Dummy Variable Trap üü¢\n",
        "\n",
        "## üéØ Interview Prep: Encoding Logic\n",
        "**Context:** Understanding why \"less is more\" when converting categorical strings into numerical features for linear models.\n",
        "\n",
        "\n",
        "### üìö The Core Question\n",
        "> **Scenario:** You are encoding a categorical variable with **5 categories** (e.g., \"Monday\" through \"Friday\"). How many dummy variables should you create?\n",
        "\n",
        "**The Short Answer:** You should create **4** dummy variables.\n",
        "\n",
        "\n",
        "### üîç Explaining the \"Dummy Variable Trap\"\n",
        "In an interview, you should explain the **why** behind the $N-1$ rule:\n",
        "\n",
        "1.  **Perfect Multicollinearity:** If you include all $5$ columns, one column can be predicted perfectly by the others. For example, if it is not Monday, Tuesday, Wednesday, or Thursday, it *must* be Friday.\n",
        "2.  **Mathematical Conflict:** In linear regression, this perfect relationship makes the design matrix \"singular\" (non-invertible). This prevents the algorithm from finding a unique solution for the coefficients.\n",
        "3.  **The Reference Category:** The category you drop becomes the \"reference\" or \"baseline.\" The coefficients of the remaining 4 variables then represent the difference from that baseline.\n",
        "\n",
        "\n",
        "### üõ†Ô∏è Implementation Fix\n",
        "When using `pandas` or `scikit-learn`, you must explicitly trigger the drop:\n",
        "\n",
        "```python\n",
        "# Using Pandas\n",
        "df_encoded = pd.get_dummies(df, columns=['Weekday'], drop_first=True)\n",
        "\n",
        "# Using Scikit-Learn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD6FwbUhoRWN"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "data = pd.DataFrame({\n",
        "    'City': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix', 'NYC', 'LA', 'Chicago'],\n",
        "    'Sales': [100, 150, 120, 130, 110, 95, 155, 125]\n",
        "})\n",
        "\n",
        "print(f\"Unique cities: {data['City'].nunique()}\")\n",
        "\n",
        "# Correct Encoding to avoid the trap\n",
        "encoded_data = pd.get_dummies(data, columns=['City'], drop_first=True)\n",
        "print(\"\\nEncoded Data (with drop_first=True):\")\n",
        "print(encoded_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7WQLh8aoRWN"
      },
      "source": [
        "# Solution: Problem 1.2 - The Dummy Variable Trap üü¢\n",
        "\n",
        "## 1. Implementation: The Wrong vs. Correct Way\n",
        "\n",
        "Using the dataset provided, here is how both approaches look in practice:\n",
        "\n",
        "```python\n",
        "# 1. The WRONG way (including all columns)\n",
        "wrong_way = pd.get_dummies(data, columns=['City'], drop_first=False)\n",
        "\n",
        "# 2. The CORRECT way (avoiding the trap)\n",
        "correct_way = pd.get_dummies(data, columns=['City'], drop_first=True)\n",
        "\n",
        "print(\"Wrong Way columns:\", wrong_way.columns.tolist())\n",
        "print(\"Correct Way columns:\", correct_way.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EILn57LsoRWN"
      },
      "outputs": [],
      "source": [
        "# Includes all 5 city columns\n",
        "wrong_way = pd.get_dummies(data, columns=['City'], drop_first=False)\n",
        "\n",
        "print(\"Columns created (Wrong):\", [col for col in wrong_way.columns if 'City' in col])\n",
        "# Result: ['City_Chicago', 'City_Houston', 'City_LA', 'City_NYC', 'City_Phoenix']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ogs3o45oRWN"
      },
      "source": [
        "# üü¢ Problem 1.3: Detecting and Handling Outliers\n",
        "\n",
        "## üéØ Interview Prep: Scaler Selection\n",
        "**Context:** Choosing the right scaling strategy when your data is \"dirty\" or contains extreme values that might skew a standard distribution.\n",
        "\n",
        "### üìö The Scenario\n",
        "> **Interview Question:** \"If your dataset has significant outliers, would you still use `StandardScaler`? What are the alternatives?\"\n",
        "\n",
        "#### 1. The Problem with `StandardScaler`\n",
        "`StandardScaler` uses the **Mean** and **Standard Deviation**. Both of these metrics are highly sensitive to outliers. A single extreme value can \"pull\" the mean away from the center of the data and inflate the standard deviation, causing the majority of your data to be squeezed into a very small range after scaling.\n",
        "\n",
        "#### 2. The Alternative: `RobustScaler`\n",
        "`RobustScaler` is specifically designed for this. Instead of the mean, it uses the **Median**. Instead of standard deviation, it uses the **Interquartile Range (IQR)**.\n",
        "* **Median:** The middle value (unaffected by how high the highest value is).\n",
        "* **IQR:** The range between the 25th and 75th percentiles (captures the \"core\" of your data).\n",
        "\n",
        "### üõ†Ô∏è Code Comparison\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "\n",
        "# Simulated data with a massive outlier\n",
        "X = np.array([10, 12, 11, 13, 12, 11, 1000]).reshape(-1, 1)\n",
        "\n",
        "# StandardScaler result\n",
        "ss = StandardScaler().fit_transform(X)\n",
        "# The majority of values will be ~ -0.3, while 1000 becomes ~ 2.6\n",
        "\n",
        "# RobustScaler result\n",
        "rs = RobustScaler().fit_transform(X)\n",
        "# The majority of values stay around 0, correctly identifying 1000 as a massive outlier (e.g., 494.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nEnBd2WoRWO"
      },
      "source": [
        "# Problem 1.3: Quick Imputation Decision üü¢\n",
        "\n",
        "## üéØ Interview Prep: Handling Missingness\n",
        "**Context:** Identifying the best recovery strategy for missing data based on the variable type and distribution.\n",
        "\n",
        "\n",
        "### üìö The Decision Matrix\n",
        "\n",
        "> **Interview Question:** *\"How do you decide which imputation method to use for a specific feature?\"*\n",
        "\n",
        "In an interview, don't just say \"fill with the mean.\" Use the following logic to demonstrate a nuanced understanding:\n",
        "\n",
        "| Feature Type | Distribution | Best Imputation Method | Why? |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Numeric** | Normal (Symmetric) | **Mean** | The average is the best central representation of Gaussian data. |\n",
        "| **Numeric** | Skewed / Outliers | **Median** | The median is robust and isn't pulled by extreme values. |\n",
        "| **Categorical** | Any | **Mode** (Most Frequent) | You cannot average \"New York\" and \"LA\"; you pick the most common class. |\n",
        "| **Any** | Pattern-based | **KNN / MICE** | Uses other features to \"predict\" the missing value (Advanced). |\n",
        "\n",
        "### üõ†Ô∏è Strategic Decision Flow\n",
        "\n",
        "1.  **Is it Categorical?** ‚Üí Use **Mode** or create a new category called **\"Missing\"**.\n",
        "2.  **Is it Numeric?** ‚Üí Check for skewness or outliers.\n",
        "    * No outliers? ‚Üí **Mean**.\n",
        "    * Significant outliers? ‚Üí **Median**.\n",
        "3.  **Is the missingness \"Not at Random\" (MNAR)?**\n",
        "    * If the fact that it's missing is a signal (e.g., people with high debt don't report it), add a **Binary Indicator Column** (`is_missing`) to tell the model that the data was absent.\n",
        "\n",
        "\n",
        "### üöÄ Interview Soundbite\n",
        "> \"My choice depends on the data distribution. For normally distributed numeric data, I use the **mean**. However, if the data is skewed or has outliers, I prefer the **median** because it's more robust. For categorical data, I default to the **mode** or create a dedicated 'Missing' category to preserve the signal that the information was unavailable.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKMylKU2oRWO"
      },
      "outputs": [],
      "source": [
        "# IMPLEMENTATION STRATEGY\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Dataset 1 (Normal) -> Mean\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "data1_imputed = imputer_mean.fit_transform(data1.reshape(-1, 1))\n",
        "\n",
        "# Dataset 2 (Skewed) -> Median\n",
        "imputer_median = SimpleImputer(strategy='median')\n",
        "data2_imputed = imputer_median.fit_transform(data2.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO5UFbmmoRWO"
      },
      "source": [
        "# Solution: Problem 1.3 - Imputation Decision Matrix üü¢\n",
        "\n",
        "## üéØ Interview Prep: Distribution-Based Imputation\n",
        "**Context:** Demonstrating that your preprocessing steps are grounded in the statistical properties of the feature, rather than a \"one-size-fits-all\" approach.\n",
        "\n",
        "\n",
        "### üìö The Decision Matrix\n",
        "\n",
        "| Dataset | Best Method | Reasoning |\n",
        "| :--- | :--- | :--- |\n",
        "| **Dataset 1** | **Mean** | This is a **Normal (Gaussian) distribution**. In symmetric data without heavy tails, the mean is the most statistically efficient estimator of the central tendency. |\n",
        "| **Dataset 2** | **Median** | This is **Heavily Right-Skewed (Exponential)**. The mean would be \"pulled\" to the right by the long tail, leading to biased imputation. The median is robust to skewness and outliers. |\n",
        "| **Dataset 3** | **Median (or Mean)** | This is a **Uniform distribution**. While both are mathematically similar here, **Median** is often the safer \"production\" choice as it remains robust if future incoming data contains unexpected extreme values. |\n",
        "\n",
        "\n",
        "### üîç Deep Dive: Why the choice matters\n",
        "In an interview, use these specific justifications to show depth:\n",
        "\n",
        "* **For Dataset 2 (Skewed):** Explain that using the mean would result in **imputing values that are too high** relative to the majority of the data. This creates a \"bump\" in the distribution that doesn't actually exist in the population.\n",
        "* **The Outlier Factor:** If Dataset 1 suddenly had an extreme outlier (e.g., a value of 500), the **Mean** would shift significantly, while the **Median** would remain stable. This is why many practitioners default to Median for numeric data unless they are certain the data is strictly Gaussian.\n",
        "\n",
        "\n",
        "### üöÄ Interview Soundbite\n",
        "> \"For Dataset 1, I‚Äôd use the **mean** because the data is symmetric and normally distributed. However, for Dataset 2, the exponential nature creates a heavy right skew; in this case, the mean is a poor representation of the 'typical' value, so I‚Äôd use the **median** to avoid introducing bias from the long tail.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pMTZGwDoRWO"
      },
      "outputs": [],
      "source": [
        "# Your answer:\n",
        "\n",
        "# Dataset 1:\n",
        "\n",
        "# Dataset 2:\n",
        "\n",
        "# Dataset 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOb8darmoRWO"
      },
      "source": [
        "## Part 2: Intermediate Problems üü°\n",
        "\n",
        "### Problem 2.1: Mutual Information vs. Correlation\n",
        "\n",
        "**Scenario:** You have a dataset where a feature has a perfect **quadratic relationship** with the target ().\n",
        "\n",
        "* **Question:** If you use **Pearson Correlation** for feature selection, what will happen? What should you use instead?\n",
        "\n",
        "**Interview Focus:** This tests if you understand the mathematical limitations of linear metrics versus information-theoretic ones.\n",
        "\n",
        "### üõ†Ô∏è Implementation & Demonstration\n",
        "\n",
        "Let's simulate this scenario to see how the two metrics perform:\n",
        "\n",
        "```python\n",
        "# Generate non-linear data\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = x**2 + np.random.normal(0, 5, 100) # Quadratic with noise\n",
        "\n",
        "# Calculate Correlation\n",
        "correlation = np.corrcoef(x, y)[0, 1]\n",
        "\n",
        "# Calculate Mutual Information\n",
        "# (Note: MI requires a 2D array for X)\n",
        "mi_score = mutual_info_classif(x.reshape(-1, 1), (y > y.mean()).astype(int))[0]\n",
        "\n",
        "print(f\"Pearson Correlation: {correlation:.4f}\")\n",
        "print(f\"Mutual Information Score: {mi_score:.4f}\")\n",
        "\n",
        "### üìö The Solution\n",
        "\n",
        "1. **The Result:** Pearson Correlation will be close to **0**. Because the relationship is a \"U-shape\" (symmetric around the y-axis), the linear \"best fit\" line is horizontal, suggesting no relationship at all.\n",
        "2. **The Problem:** Correlation only measures **linear** dependencies. It is \"blind\" to non-linear patterns.\n",
        "3. **The Fix:** Use **Mutual Information (MI)**. MI measures how much information the presence of one variable provides about the other. It captures any kind of statistical dependency (linear or non-linear).\n",
        "\n",
        "### üöÄ Interview Soundbite\n",
        "\n",
        "> \"Pearson correlation is a great first pass, but it only captures linear relationships. In cases with complex patterns‚Äîlike quadratic or periodic relationships‚Äîit can return a score of zero even if the feature is highly predictive. For a more robust feature selection, I prefer **Mutual Information**, as it uses entropy to capture any form of dependency between the feature and the target.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnOl-dYoRWP"
      },
      "source": [
        "In a real-world scenario with 50 features, jumping straight to a complex model can lead to overfitting and high computational costs. An interviewer wants to see a **tiered approach**‚Äîmoving from fast, broad filters to precise, model-based wrappers.\n",
        "\n",
        "### üõ†Ô∏è The \"Filter-to-Wrapper\" Strategy\n",
        "\n",
        "1. **Filter Stage (Fast):** Remove constant features (variance = 0) and highly correlated features to reduce redundancy.\n",
        "2. **Statistical Stage (Medium):** Use **Mutual Information** or **ANOVA F-test** to rank features based on their individual relationship with the target.\n",
        "3. **Wrapper Stage (Slow but Precise):** Use **Recursive Feature Elimination (RFE)** with a model like Random Forest to capture feature interactions.\n",
        "\n",
        "\n",
        "### üíª Implementation: A Multi-Stage Pipeline\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1. Initialize the components\n",
        "# Use Mutual Information to narrow down 50 features to 25 (Filter)\n",
        "filter_selector = SelectKBest(score_func=mutual_info_classif, k=25)\n",
        "\n",
        "# 2. Use RFE with a Random Forest to find the final 'Top 10' (Wrapper)\n",
        "estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "wrapper_selector = RFE(estimator=estimator, n_features_to_select=10, step=1)\n",
        "\n",
        "# 3. Combine into a pipeline\n",
        "selection_pipeline = Pipeline([\n",
        "    ('filter', filter_selector),\n",
        "    ('wrapper', wrapper_selector)\n",
        "])\n",
        "\n",
        "# selection_pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "### üîç Interview Explanation: The Trade-offs\n",
        "\n",
        "| Method | Why use it? | The Trade-off |\n",
        "| --- | --- | --- |\n",
        "| **Filter (MI)** | Extremely fast; handles non-linear relationships. | Ignores interactions between features (evaluates features in isolation). |\n",
        "| **Wrapper (RFE)** | Captures how features work *together* to help a model. | Computationally expensive; prone to overfitting if the dataset is small. |\n",
        "\n",
        "\n",
        "### üöÄ Interview Soundbite\n",
        "\n",
        "> \"To build a robust feature selection pipeline, I start with a **Filter method** like Mutual Information to quickly discard noise and non-informative features. Then, I apply a **Wrapper method** like RFE with a Random Forest. This two-step approach is more efficient than running RFE on all 50 features at once, as it balances computational speed with the ability to detect complex feature interactions.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyMjdiKBoRWP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# --- STAGE 1: Filter Method ---\n",
        "# Select top 25 features based on Mutual Information\n",
        "filter_selector = SelectKBest(score_func=mutual_info_classif, k=25)\n",
        "X_filter = filter_selector.fit_transform(X, y)\n",
        "\n",
        "# --- STAGE 2: Wrapper Method ---\n",
        "# Use RFE to get down to the final 15 features\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfe = RFE(estimator=rfc, n_features_to_select=15)\n",
        "X_final = rfe.fit_transform(X_filter, y)\n",
        "\n",
        "print(f\"Final feature shape: {X_final.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCv-GgW5oRWP"
      },
      "source": [
        "### üíª 3-Step Selection Pipeline\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif, RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Filter low-variance features\n",
        "selector_var = VarianceThreshold(threshold=0.01)\n",
        "X_var = selector_var.fit_transform(X)\n",
        "\n",
        "# Step 2: Select top 20 via Mutual Information\n",
        "selector_k = SelectKBest(score_func=mutual_info_classif, k=20)\n",
        "X_k = selector_k.fit_transform(X_var, y)\n",
        "\n",
        "# Step 3: RFE with Logistic Regression to get final 10\n",
        "estimator = LogisticRegression(solver='liblinear')\n",
        "selector_rfe = RFE(estimator=estimator, n_features_to_select=10)\n",
        "X_final = selector_rfe.fit_transform(X_k, y)\n",
        "\n",
        "# Performance Comparison\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "score_all = cross_val_score(model, X, y, cv=5).mean()\n",
        "score_sel = cross_val_score(model, X_final, y, cv=5).mean()\n",
        "\n",
        "print(f\"Step 1 (Variance): {X_var.shape[1]} features remaining\")\n",
        "print(f\"Step 2 (KBest):    {X_k.shape[1]} features remaining\")\n",
        "print(f\"Step 3 (RFE):      {X_final.shape[1]} features remaining\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy (All 50):      {score_all:.4f}\")\n",
        "print(f\"Accuracy (Selected 10): {score_sel:.4f}\")\n",
        "```\n",
        "\n",
        "### üìä Visualization & Analysis\n",
        "\n",
        "#### Why this sequence works for an Interview Answer:\n",
        "\n",
        "1. **Variance Threshold (The Janitor):** We remove features that are essentially constant. If a feature doesn't change, it contains no information to help a model distinguish between classes.\n",
        "2. **SelectKBest (The Filter):** Using **Mutual Information** is crucial here because it captures both linear and non-linear relationships. It's much faster than RFE, allowing us to cut the feature space in half almost instantly.\n",
        "3. **RFE (The Specialist):** Recursive Feature Elimination considers feature **interactions**. By the time we reach this step, we are only asking the model to evaluate the \"best of the best,\" making the computation much lighter.\n",
        "\n",
        "### üöÄ Key Findings\n",
        "\n",
        "* **Dimensionality Reduction:** We reduced the feature space by **80%** (50 down to 10).\n",
        "* **Performance:** You will often find that the \"Selected 10\" performs nearly as well as (or sometimes better than) the \"All 50.\" This happens because we've removed the **noise** and **redundant** features that lead to overfitting.\n",
        "* **The \"Curse\":** By keeping only the most informative features, we improve the \"signal-to-noise\" ratio, which is vital when working with smaller datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEVZeS1roRWP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif, RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# =================================================================\n",
        "# STEP 1: Filter Method - Variance Threshold\n",
        "# Goal: Remove \"boring\" features that don't change enough to be useful.\n",
        "# =================================================================\n",
        "\n",
        "# 1. Initialize the selector (threshold=0.01 means features must vary\n",
        "#    by more than 1% to be kept)\n",
        "selector_var = VarianceThreshold(threshold=0.01)\n",
        "\n",
        "# 2. Fit and transform the dataset\n",
        "# X_var = ???\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# STEP 2: Filter Method - Statistical Ranking (SelectKBest)\n",
        "# Goal: Use Mutual Information to find the top 20 features based\n",
        "#       on their relationship with the target 'y'.\n",
        "# =================================================================\n",
        "\n",
        "# 1. Initialize SelectKBest using mutual_info_classif\n",
        "# selector_k = SelectKBest(score_func=???, k=20)\n",
        "\n",
        "# 2. Fit and transform the data from Step 1\n",
        "# X_k = ???\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# STEP 3: Wrapper Method - Recursive Feature Elimination (RFE)\n",
        "# Goal: Use a model to find the final 10 features that work best\n",
        "#       together by iteratively removing the least important ones.\n",
        "# =================================================================\n",
        "\n",
        "# 1. Choose a base model for RFE to use for feature ranking\n",
        "estimator = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# 2. Initialize RFE to select the top 10 features\n",
        "# selector_rfe = RFE(estimator=???, n_features_to_select=10)\n",
        "\n",
        "# 3. Fit and transform the data from Step 2\n",
        "# X_final = ???\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# PERFORMANCE EVALUATION\n",
        "# Goal: Compare the \"All Features\" model vs. the \"Selected Features\"\n",
        "#       model using 5-fold cross-validation.\n",
        "# =================================================================\n",
        "\n",
        "# model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# 1. Calculate accuracy for the original X (50 features)\n",
        "# score_all = cross_val_score(model, X, y, cv=5).mean()\n",
        "\n",
        "# 2. Calculate accuracy for X_final (10 features)\n",
        "# score_selected = cross_val_score(model, X_final, y, cv=5).mean()\n",
        "\n",
        "# print(f\"Accuracy with all features: {score_all:.4f}\")\n",
        "# print(f\"Accuracy with selected features: {score_selected:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6i4FiMcoRWP"
      },
      "source": [
        "### Problem 2.2: PCA Scree Plot Analysis\n",
        "\n",
        "**Scenario**\n",
        "You are presenting PCA results to stakeholders. They ask:\n",
        "‚ÄúHow many components should we use?‚Äù\n",
        "\n",
        "**Interview Skill Assessed**\n",
        "Interpreting PCA results and clearly communicating decisions to a non-technical audience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES1uWC5noRWQ"
      },
      "outputs": [],
      "source": [
        "# Load wine dataset (13 features)\n",
        "wine = load_wine()\n",
        "X_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y_wine = wine.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_wine_scaled = scaler.fit_transform(X_wine)\n",
        "\n",
        "# Fit PCA with all components\n",
        "pca = PCA()\n",
        "pca.fit(X_wine_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot scree plot and cumulative variance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scree plot\n",
        "axes[0].bar(\n",
        "    range(1, len(explained_variance) + 1),\n",
        "    explained_variance,\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "axes[0].set_xlabel(\"Principal Component\")\n",
        "axes[0].set_ylabel(\"Explained Variance Ratio\")\n",
        "axes[0].set_title(\"Scree Plot\")\n",
        "\n",
        "# Cumulative explained variance\n",
        "axes[1].plot(\n",
        "    range(1, l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7-MCpszoRWQ"
      },
      "source": [
        "**Your Tasks**\n",
        "\n",
        "1. Using the elbow method, how many components would you choose?\n",
        "2. How many components are needed to explain 90% of the variance? How many for 95%?\n",
        "3. Write a 2‚Äì3 sentence explanation of your recommendation for a non-technical manager.\n",
        "4. What is the trade-off between using fewer versus more components?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUm-RTxqoRWQ"
      },
      "outputs": [],
      "source": [
        "# Your answers\n",
        "\n",
        "# 1. Elbow method choice\n",
        "# ANSWER:\n",
        "\n",
        "# 2. Components needed for variance thresholds\n",
        "# 90% variance:\n",
        "# 95% variance:\n",
        "\n",
        "# 3. Explanation for a non-technical manager\n",
        "# ANSWER:\n",
        "\n",
        "# 4. Trade-offs between fewer vs. more components\n",
        "# ANSWER:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyGIIlfYoRWQ"
      },
      "source": [
        "### Solution\n",
        "\n",
        "**1. Elbow method**\n",
        "From the scree plot, the elbow appears around principal components 2‚Äì3.\n",
        "\n",
        "**2. Components needed for variance thresholds**\n",
        "\n",
        "* For 90% variance: approximately 5 components\n",
        "* For 95% variance: approximately 6 components\n",
        "\n",
        "**3. Explanation for a non-technical manager**\n",
        "I recommend using 5‚Äì6 principal components. This captures 90‚Äì95% of the information in the data while reducing the number of features from 13 to about 5‚Äì6. The result is a simpler, faster model with very little loss of useful information.\n",
        "\n",
        "**4. Trade-offs between fewer vs. more components**\n",
        "\n",
        "*Fewer components (2‚Äì3)*\n",
        "\n",
        "* Easier to visualize\n",
        "* Faster computation\n",
        "* May discard important information\n",
        "\n",
        "*More components (10+)*\n",
        "\n",
        "* Preserves more information\n",
        "* Reduces the benefit of dimensionality reduction\n",
        "* Harder to interpret\n",
        "\n",
        "**Interview tip**\n",
        "Always tie technical decisions back to business value, such as speed, simplicity, and interpretability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvSwGeezoRWQ"
      },
      "source": [
        "## Part 3: Advanced Problems\n",
        "\n",
        "### Problem 3.1: Complete Preprocessing Pipeline\n",
        "\n",
        "**Scenario**\n",
        "This is a realistic ‚Äúmessy data‚Äù interview question. You are given a dataset with:\n",
        "\n",
        "* Mixed numeric and categorical features\n",
        "* Missing values\n",
        "* Features on different scales\n",
        "* High dimensionality\n",
        "\n",
        "**Task**\n",
        "Design and implement a complete preprocessing pipeline that prepares the data for modeling.\n",
        "\n",
        "**Why this matters**\n",
        "This is a very common take-home and on-site interview problem. The goal is to demonstrate both technical correctness and good data engineering judgment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXrB3rOWoRWQ"
      },
      "outputs": [],
      "source": [
        "# Create a realistic messy dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "\n",
        "messy_data = pd.DataFrame({\n",
        "    \"age\": np.random.randint(18, 80, n_samples),\n",
        "    \"income\": np.random.exponential(50_000, n_samples),\n",
        "    \"credit_score\": np.random.normal(700, 50, n_samples),\n",
        "    \"years_employed\": np.random.randint(0, 40, n_samples),\n",
        "    \"education\": np.random.choice([\"HS\", \"BS\", \"MS\", \"PhD\"], n_samples),\n",
        "    \"city\": np.random.choice(\n",
        "        [\"NYC\", \"LA\", \"Chicago\", \"Houston\", \"Phoenix\"],\n",
        "        n_samples\n",
        "    ),\n",
        "    \"loan_approved\": np.random.choice([0, 1], n_samples)\n",
        "})\n",
        "\n",
        "# Introduce missing values\n",
        "messy_data.loc[\n",
        "    np.random.choice(n_samples, 20, replace=False),\n",
        "    \"income\"\n",
        "] = np.nan\n",
        "\n",
        "messy_data.loc[\n",
        "    np.random.choice(n_samples, 15, replace=False),\n",
        "    \"credit_score\"\n",
        "] = np.nan\n",
        "\n",
        "messy_data.loc[\n",
        "    np.random.choice(n_samples, 10, replace=False),\n",
        "    \"education\"\n",
        "] = np.nan\n",
        "\n",
        "# Introduce outliers\n",
        "messy_data.loc[\n",
        "    np.random.choice(n_samples, 5, replace=False),\n",
        "    \"income\"\n",
        "] = np.random.uniform(500_000, 1_000_000, 5)\n",
        "\n",
        "# Inspect dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(messy_data.info())\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(messy_data.isnull().sum())\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(messy_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVukVWQUoRWQ"
      },
      "source": [
        "**Your Task**\n",
        "\n",
        "Create a preprocessing function that:\n",
        "\n",
        "1. Handles missing values appropriately for each feature\n",
        "2. Detects and treats outliers in numeric features\n",
        "3. Encodes categorical variables correctly\n",
        "4. Scales numeric features\n",
        "5. Returns a clean dataset ready for modeling\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "* Must work on both training and test data\n",
        "* Document your design decisions\n",
        "* Explain any trade-offs you make\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXb3LZJioRWR"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, is_train=True, scaler=None, encoders=None):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline for messy data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Input data\n",
        "    is_train : bool\n",
        "        If True, fit scalers/encoders.\n",
        "        If False, use the provided ones.\n",
        "    scaler : fitted scaler or None\n",
        "    encoders : dict of fitted encoders or None\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_processed : pandas.DataFrame\n",
        "        Cleaned and transformed data\n",
        "    scaler : fitted scaler\n",
        "    encoders : dict of fitted encoders\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# Test your function\n",
        "\n",
        "X = messy_data.drop(\"loan_approved\", axis=1)\n",
        "y = messy_data[\"loan_approved\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Process training data\n",
        "X_train_processed, scaler, encoders = preprocess_data(\n",
        "    X_train,\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "# Process test data (using training parameters)\n",
        "X_test_processed, _, _ = preprocess_data(\n",
        "    X_test,\n",
        "    is_train=False,\n",
        "    scaler=scaler,\n",
        "    encoders=encoders\n",
        ")\n",
        "\n",
        "print(\"Processed training shape:\", X_train_processed.shape)\n",
        "print(\"Processed test shape:\", X_test_processed.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6F0wk0xoRWR"
      },
      "source": [
        "### Hints\n",
        "\n",
        "**Suggested approach**\n",
        "\n",
        "1. Separate numeric and categorical columns.\n",
        "\n",
        "2. For numeric features:\n",
        "\n",
        "   * Inspect the distribution before choosing an imputation strategy\n",
        "   * Detect outliers using the IQR method\n",
        "   * Apply scaling only after missing values are handled\n",
        "\n",
        "3. For categorical features:\n",
        "\n",
        "   * Impute missing values using the mode\n",
        "   * One-hot encode categories, using `drop_first=True`\n",
        "\n",
        "4. Combine the processed numeric and categorical features into a single dataset.\n",
        "\n",
        "**Key insight**\n",
        "When `is_train=True`, always save the fitted transformers (imputation values, outlier bounds, scalers, encoders). Reuse them on test data to prevent data leakage.\n",
        "\n",
        "### Solution Overview (Conceptual)\n",
        "\n",
        "The preprocessing function follows a structured, production-ready workflow:\n",
        "\n",
        "* A copy of the input data is created to avoid modifying the original dataset.\n",
        "* Numeric and categorical columns are identified automatically.\n",
        "\n",
        "**Numeric features**\n",
        "\n",
        "* Missing values are imputed using the median, which is robust to skewed distributions and outliers.\n",
        "* Outliers are capped using the interquartile range (IQR) method rather than removed.\n",
        "* All imputation values and bounds are saved during training and reused during testing.\n",
        "\n",
        "**Categorical features**\n",
        "\n",
        "* Missing values are imputed using the most frequent category (mode).\n",
        "* Categorical variables are converted to numeric form using one-hot encoding.\n",
        "\n",
        "**Scaling**\n",
        "\n",
        "* Numeric features are standardized using a scaler fitted on the training data only.\n",
        "* The same scaler is reused for test data to ensure consistency.\n",
        "\n",
        "The final output is a clean, fully numeric dataset with no missing values and consistent preprocessing between training and testing.\n",
        "\n",
        "\n",
        "### Interview Talking Points\n",
        "\n",
        "* Median imputation is chosen because features like income are often right-skewed.\n",
        "* IQR-based capping limits extreme values while preserving overall information.\n",
        "* Persisting fitted transformers avoids data leakage between train and test sets.\n",
        "* The approach mirrors real-world, production-ready preprocessing pipelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx3Z35YJoRWR"
      },
      "source": [
        "### Problem 3.2: When PCA Goes Wrong\n",
        "\n",
        "**Scenario**\n",
        "A colleague applied PCA and obtained very poor results. Your task is to diagnose what went wrong.\n",
        "\n",
        "**Interview Skill Assessed**\n",
        "Critical thinking and knowing when *not* to apply a particular technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhDuThxXoRWR"
      },
      "outputs": [],
      "source": [
        "# Your colleague's code\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create dataset with clear class separation\n",
        "X, y = make_classification(\n",
        "    n_samples=300,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    flip_y=0.05,\n",
        "    class_sep=3.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=1)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Original data\n",
        "axes[0].scatter(\n",
        "    X[y == 0, 0],\n",
        "    X[y == 0, 1],\n",
        "    label=\"Class 0\",\n",
        "    alpha=0.6\n",
        ")\n",
        "axes[0].scatter(\n",
        "    X[y == 1, 0],\n",
        "    X[y == 1, 1],\n",
        "    label=\"Class 1\",\n",
        "    alpha=0.6\n",
        ")\n",
        "axes[0].set_title(\"Original Data (2D)\")\n",
        "axes[0].set_xlabel(\"Feature 1\")\n",
        "axes[0].set_ylabel(\"Feature 2\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# After PCA\n",
        "axes[1].scatter(\n",
        "    X_pca[y == 0],\n",
        "    np.zeros(sum(y == 0)),\n",
        "    label=\"Class 0\",\n",
        "    alpha=0.6\n",
        ")\n",
        "axes[1].scatter(\n",
        "    X_pca[y == 1],\n",
        "    np.zeros(sum(y == 1)),\n",
        "    label=\"Class 1\",\n",
        "    alpha=0.6\n",
        ")\n",
        "axes[1].set_title(\"After PCA (1D)\")\n",
        "axes[1].set_xlabel(\"PC1\")\n",
        "axes[1].set_yticks([])\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Train models\n",
        "model_original = SVC(kernel=\"linear\", random_state=42)\n",
        "model_pca = SVC(kernel=\"linear\", random_state=42)\n",
        "\n",
        "model_original.fit(X, y)\n",
        "model_pca.fit(X_pca.reshape(-1, 1), y)\n",
        "\n",
        "print(\n",
        "    f\"Accuracy with original features: \"\n",
        "    f\"{model_original.score(X, y):.3f}\"\n",
        ")\n",
        "print(\n",
        "    f\"Accuracy with PCA: \"\n",
        "    f\"{model_pca.score(X_pca.reshape(-1, 1), y):.3f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRsHdF19oRWS"
      },
      "source": [
        "**Your Tasks**\n",
        "\n",
        "1. **Identify the problem**\n",
        "   Why did PCA hurt model performance in this case?\n",
        "\n",
        "2. **Recommend an alternative**\n",
        "   What dimensionality reduction technique would you use instead, and why?\n",
        "\n",
        "3. **When is PCA appropriate?**\n",
        "   Provide clear guidelines for when PCA should (and should not) be used.\n",
        "\n",
        "**Concept Check**\n",
        "This question tests whether you understand the fundamental difference between PCA and Linear Discriminant Analysis (LDA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz3ByNDDoRWS"
      },
      "outputs": [],
      "source": [
        "# Your answers\n",
        "\n",
        "# 1. Why PCA failed\n",
        "# ANSWER:\n",
        "\n",
        "# 2. Better dimensionality reduction technique\n",
        "# CODE:\n",
        "\n",
        "# 3. When to use PCA\n",
        "# ANSWER:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwaKhmoVoRWS"
      },
      "source": [
        "### Solution\n",
        "\n",
        "**1. Why PCA failed**\n",
        "PCA identifies directions of maximum variance, but those directions do not necessarily separate classes.\n",
        "In this example, the first principal component captures overall variance but projects both classes onto overlapping regions, which harms classification performance.\n",
        "\n",
        "**2. Better technique: Linear Discriminant Analysis (LDA)**\n",
        "LDA is a supervised dimensionality reduction method. Unlike PCA, it explicitly uses class labels and finds projections that maximize separation between classes while minimizing variance within each class.\n",
        "As a result, LDA preserves class-discriminative information that PCA may discard.\n",
        "\n",
        "**3. When to use PCA**\n",
        "\n",
        "PCA is appropriate when:\n",
        "\n",
        "* Performing unsupervised learning or clustering\n",
        "* Visualizing high-dimensional data\n",
        "* Reducing multicollinearity\n",
        "* Removing noise\n",
        "\n",
        "PCA should be avoided when:\n",
        "\n",
        "* The goal is maximizing class separation (use LDA instead)\n",
        "* Features are on different scales and have not been standardized\n",
        "\n",
        "**Interview-ready takeaway**\n",
        "‚ÄúPCA is an unsupervised technique that focuses on variance, not class separation. When labels are available and the goal is classification, LDA is often a better choice because it directly maximizes class separability.‚Äù\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GErlDKf1oRWS"
      },
      "source": [
        "## Part 4: Challenge Problems\n",
        "\n",
        "These are timed interview pressure tests. Set a timer and work under interview conditions.\n",
        "\n",
        "### Challenge 4.1: The Feature Engineering Sprint (25 minutes)\n",
        "\n",
        "**Scenario**\n",
        "You are in a live coding interview. The interviewer gives you a dataset and says:\n",
        "‚ÄúWe've tried models on this data with about 65% accuracy. Can you engineer features to improve it? You have 25 minutes.‚Äù\n",
        "\n",
        "**Rules**\n",
        "\n",
        "* Time limit: 25 minutes\n",
        "* Must improve baseline accuracy\n",
        "* Explain your reasoning as you code\n",
        "* Be prepared to defend your choices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB8JJIq5oRWW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Dataset: Employee attrition prediction\n",
        "np.random.seed(42)\n",
        "n = 500\n",
        "\n",
        "employee_data = pd.DataFrame({\n",
        "    \"age\": np.random.randint(22, 65, n),\n",
        "    \"years_at_company\": np.random.randint(0, 40, n),\n",
        "    \"monthly_income\": np.random.normal(6000, 2000, n),\n",
        "    \"distance_from_home\": np.random.randint(1, 50, n),\n",
        "    \"num_companies_worked\": np.random.randint(0, 10, n),\n",
        "    \"years_since_last_promotion\": np.random.randint(0, 15, n),\n",
        "    \"department\": np.random.choice([\"Sales\", \"IT\", \"HR\", \"Marketing\"], n),\n",
        "    \"education_level\": np.random.randint(1, 6, n),      # 1‚Äì5 scale\n",
        "    \"job_satisfaction\": np.random.randint(1, 5, n),     # 1‚Äì4 scale\n",
        "    \"work_life_balance\": np.random.randint(1, 5, n),    # 1‚Äì4 scale\n",
        "})\n",
        "\n",
        "# Target (somewhat correlated with features)\n",
        "employee_data[\"attrition\"] = (\n",
        "    (employee_data[\"job_satisfaction\"] < 2).astype(int) * 0.4\n",
        "    + (employee_data[\"years_since_last_promotion\"] > 5).astype(int) * 0.3\n",
        "    + (employee_data[\"monthly_income\"] < 4000).astype(int) * 0.2\n",
        "    + np.random.random(n) * 0.1\n",
        ")\n",
        "employee_data[\"attrition\"] = (employee_data[\"attrition\"] > 0.5).astype(int)\n",
        "\n",
        "print(\"Dataset shape:\", employee_data.shape)\n",
        "print(\"\\nAttrition distribution:\")\n",
        "print(employee_data[\"attrition\"].value_counts(normalize=True))\n",
        "\n",
        "# Baseline model\n",
        "X_baseline = employee_data.drop(\"attrition\", axis=1)\n",
        "X_baseline = pd.get_dummies(X_baseline, drop_first=True)\n",
        "y = employee_data[\"attrition\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_baseline,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "baseline_model.fit(X_train, y_train)\n",
        "baseline_acc = baseline_model.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nBASELINE ACCURACY: {baseline_acc:.3f}\")\n",
        "print(\"\\nTimer started! You have 25 minutes.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdLrHAq0oRWX"
      },
      "source": [
        "**Your Task**\n",
        "\n",
        "Engineer features to beat the baseline model. Consider using:\n",
        "\n",
        "* Ratios and interaction features\n",
        "* Domain knowledge related to employee attrition\n",
        "* Polynomial or nonlinear transformations\n",
        "* Aggregations or derived features\n",
        "* Any other features you believe may improve predictive performance\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Document your feature engineering steps and reasoning below. Explain *why* each feature might help improve model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b77Q3W3KoRWX"
      },
      "outputs": [],
      "source": [
        "# Feature engineering (fill in below)\n",
        "\n",
        "# Some ideas to get started:\n",
        "# - age / years_at_company  -> average tenure\n",
        "# - monthly_income / age    -> income growth proxy\n",
        "# - interaction: job_satisfaction * work_life_balance\n",
        "# - flag for recent hires\n",
        "# - etc.\n",
        "\n",
        "# Your code:\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Final model with engineered features\n",
        "# -------------------------------------------------\n",
        "\n",
        "# Check time\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nTime elapsed: {elapsed/60:.1f} minutes\")\n",
        "print(f\"Baseline accuracy: {baseline_acc:.3f}\")\n",
        "print(\"Your accuracy: ???\")\n",
        "print(\"Improvement: ???\")\n",
        "``\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0xpmvsEoRWX"
      },
      "source": [
        "### Hints\n",
        "\n",
        "**Good features for attrition**\n",
        "\n",
        "1. **Career progression rate**\n",
        "   `years_at_company / (years_since_last_promotion + 1)`\n",
        "\n",
        "2. **Income relative to age (income growth proxy)**\n",
        "   `monthly_income / (age - 21 + 1)`  (assuming work starts around age 22)\n",
        "\n",
        "3. **Commute burden**\n",
        "   `distance_from_home * (1 / work_life_balance)`\n",
        "\n",
        "4. **Job hopper flag**\n",
        "   `num_companies_worked > threshold`\n",
        "\n",
        "5. **Stagnation indicator**\n",
        "   `years_since_last_promotion > years_at_company * 0.3`\n",
        "\n",
        "6. **Interaction feature**\n",
        "   `job_satisfaction * work_life_balance`\n",
        "\n",
        "### Sample Solution (Conceptual)\n",
        "\n",
        "A strong feature engineering approach here uses domain knowledge to capture:\n",
        "\n",
        "* **Career trajectory**\n",
        "\n",
        "  * Promotion rate as a measure of advancement\n",
        "  * Stagnation flags to represent lack of progression\n",
        "\n",
        "* **Compensation relative to experience**\n",
        "\n",
        "  * Income per ‚Äúworking year‚Äù to normalize income by age/experience\n",
        "  * Low income flags relative to the dataset distribution\n",
        "\n",
        "* **Lifestyle and wellbeing**\n",
        "\n",
        "  * Commute burden as a stress proxy\n",
        "  * Interaction between job satisfaction and work-life balance to capture overall wellbeing\n",
        "\n",
        "* **Risk flags**\n",
        "\n",
        "  * Recent hire, job hopper, long commute, low satisfaction indicators\n",
        "\n",
        "**Interview explanation (example)**\n",
        "‚ÄúI engineered features based on known attrition drivers: career stagnation, compensation relative to experience, and overall wellbeing. Promotion rate and stagnation capture progression, income-per-year captures compensation growth, and satisfaction‚Äìbalance interaction captures employee experience. These features translate raw fields into more predictive signals while staying interpretable.‚Äù\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o-1FMz7oRWY"
      },
      "source": [
        "### Challenge 4.2: Dimensionality Reduction Decision (20 minutes)\n",
        "\n",
        "**Scenario**\n",
        "This is a live technical interview question:\n",
        "\n",
        "‚ÄúWe have a 100-feature dataset for image classification. Recommend a dimensionality reduction approach and justify your choice.‚Äù\n",
        "\n",
        "**Interview Conditions**\n",
        "\n",
        "* Explain your reasoning clearly\n",
        "* Compare multiple approaches\n",
        "* Consider computational cost\n",
        "* Think about interpretability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8kZ5YisoRWY"
      },
      "outputs": [],
      "source": [
        "# Simulated high-dimensional image data\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()      # 8x8 pixel images = 64 features\n",
        "X_digits = digits.data\n",
        "y_digits = digits.target\n",
        "\n",
        "print(\n",
        "    f\"Dataset: {X_digits.shape[0]} samples, \"\n",
        "    f\"{X_digits.shape[1]} features, \"\n",
        "    f\"{len(np.unique(y_digits))} classes\"\n",
        ")\n",
        "\n",
        "print(\"\\nTask: Reduce from 64D to 2D for visualization and classifier input\")\n",
        "\n",
        "print(\"\\nYou must:\")\n",
        "print(\"1. Try at least 3 different dimensionality reduction techniques\")\n",
        "print(\"2. Compare them visually and quantitatively\")\n",
        "print(\"3. Make a recommendation with justification\")\n",
        "\n",
        "print(\"\\nTimer started! 20 minutes.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujbvAX-IoRWY"
      },
      "outputs": [],
      "source": [
        "# Your code goes here.\n",
        "# Implement at least 3 dimensionality reduction techniques and compare them.\n",
        "\n",
        "# Techniques to try:\n",
        "# 1) PCA\n",
        "# 2) LDA\n",
        "# 3) t-SNE\n",
        "# Bonus: Kernel PCA, Isomap, UMAP (if available), etc.\n",
        "\n",
        "# For each method:\n",
        "# - Create a 2D projection\n",
        "# - Visualize the 2D projection (scatter plot colored by class)\n",
        "# - Train a classifier on the reduced features\n",
        "# - Record accuracy and computation time\n",
        "\n",
        "# Suggested output to produce:\n",
        "# - One plot per method\n",
        "# - A short printed summary:\n",
        "#   method name, fit/transform time, classifier accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VePJ47maoRWY"
      },
      "source": [
        "### Sample Solution (Readable Summary)\n",
        "\n",
        "**Setup**\n",
        "\n",
        "* Split the digits dataset into training and test sets.\n",
        "* Standardize features using `StandardScaler`.\n",
        "* Compare three dimensionality reduction techniques:\n",
        "\n",
        "  * PCA (2 components)\n",
        "  * LDA (2 components)\n",
        "  * t-SNE (2 components, training only)\n",
        "\n",
        "\n",
        "### Methods Compared\n",
        "\n",
        "**1) PCA (Principal Component Analysis)**\n",
        "\n",
        "* Unsupervised: finds directions of maximum variance.\n",
        "* Can transform both train and test data.\n",
        "* Provides ‚Äúvariance explained,‚Äù which is useful for justification.\n",
        "\n",
        "**What to record**\n",
        "\n",
        "* Runtime for fit/transform\n",
        "* KNN test accuracy using the 2D PCA representation\n",
        "* Total variance explained by the two components\n",
        "\n",
        "\n",
        "**2) LDA (Linear Discriminant Analysis)**\n",
        "\n",
        "* Supervised: uses labels and maximizes class separation.\n",
        "* Can transform both train and test data.\n",
        "* Often strong for classification when labels are available.\n",
        "\n",
        "**What to record**\n",
        "\n",
        "* Runtime for fit/transform\n",
        "* KNN test accuracy using the 2D LDA representation\n",
        "\n",
        "**3) t-SNE**\n",
        "\n",
        "* Nonlinear visualization method (primarily for plotting).\n",
        "* Does not provide a reliable `transform()` for unseen test data in standard use.\n",
        "* Often slower and more sensitive to hyperparameters.\n",
        "\n",
        "**What to record**\n",
        "\n",
        "* Runtime for fitting on training data\n",
        "* Visualization quality (cluster separation)\n",
        "* Test accuracy is typically **not applicable** in a clean train/test pipeline\n",
        "\n",
        "### Recommendation (Interview-Ready)\n",
        "\n",
        "‚ÄúI recommend **LDA** for this classification task because it uses the labels to explicitly maximize class separability, which typically improves downstream classifier accuracy. It is also efficient and, unlike t-SNE, it can transform new (test) data consistently.\n",
        "\n",
        "If we needed an unsupervised method or cared about preserving variance without using labels, **PCA** would be my second choice.\n",
        "\n",
        "I would use **t-SNE** primarily for visualization and diagnostic insight, not as a preprocessing step for a classifier, because it does not naturally generalize to unseen data and can be computationally expensive.‚Äù\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXtqP4toRWY"
      },
      "source": [
        "## Congratulations!\n",
        "\n",
        "You've completed the Module 4 simulation scenarios.\n",
        "\n",
        "### What You've Practiced\n",
        "\n",
        "* **Warm-up:** Fundamentals (scaling, encoding, imputation)\n",
        "* **Intermediate:** Multi-step pipelines and interpretation\n",
        "* **Advanced:** Production-ready code and debugging\n",
        "* **Challenge:** Interview pressure tests and time-constrained problem solving\n",
        "\n",
        "### Interview Preparation Checklist\n",
        "\n",
        "* Can you explain when to use each scaling method?\n",
        "* Can you avoid the dummy variable trap?\n",
        "* Can you build a complete preprocessing pipeline?\n",
        "* Can you debug feature engineering mistakes?\n",
        "* Can you choose between PCA, LDA, and t-SNE appropriately?\n",
        "* Can you work under time pressure while staying organized?\n",
        "* Can you explain trade-offs to non-technical stakeholders?\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Review solutions for any problems you struggled with.\n",
        "2. Time yourself on the challenge problems again.\n",
        "3. Practice explaining your decisions out loud.\n",
        "4. Create your own variations of these problems.\n",
        "5. Review the concepts in `Interview_Prep_4_Terms_and_Concepts.pdf`.\n",
        "\n",
        "### Common Interview Mistakes to Avoid\n",
        "\n",
        "* Forgetting to scale before PCA\n",
        "* Using `fit_transform()` on test data\n",
        "* Creating the dummy variable trap\n",
        "* Not documenting preprocessing steps\n",
        "* Using t-SNE for preprocessing\n",
        "* Choosing mean instead of median for skewed data\n",
        "* Ignoring computational cost\n",
        "\n",
        "### Remember\n",
        "\n",
        "> The interviewer cares more about your thought process than perfect code. Explain your reasoning as you work.\n",
        "\n",
        "Good luck with your interviews.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}